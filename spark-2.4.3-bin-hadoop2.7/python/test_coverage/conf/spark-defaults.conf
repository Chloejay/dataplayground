#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This is used to generate PySpark coverage results. Seems there's no way to
# add a configuration when SPARK_TESTING environment variable is set because
# we will directly execute modules by python -m.
# 
#spark.python.daemon.module coverage_daemon
#spark.hadoop.fs.s3a.access.key "AKIAITQZBLIKCDU6X7SA"
#spark.hadoop.fs.s3a.secret.key "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" 

#make sure jars are added to CLASSPATH

#spark.yarn.jars=file://{spark/home/dir}/jars/*.jar,file://{hadoop/install/dir}/share/hadoop/tools/lib/*.jar


#spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem  
#spark.hadoop.fs.s3a.access.key={s3a.access.key} 
#spark.hadoop.fs.s3a.secret.key={s3a.secret.key} 

spark.jars.packages             net.java.dev.jets3t:jets3t:0.9.0,com.google.guava:guava:16.0.1,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1
spark.hadoop.fs.s3a.impl        org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.access.key   "AKIAITQZBLIKCDU6X7SA"
spark.hadoop.fs.s3a.secret.key  "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"  
spark.hadoop.fs.s3a.fast.upload true
